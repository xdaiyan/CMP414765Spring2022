{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week06_MultilinearRegression",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOelxXuXF2chdV+dzDuFuFj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ch00226855/CMP414765Spring2022/blob/main/Week06_MultilinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXhaotNQxr2w"
      },
      "source": [
        "# Week 6\n",
        "# Multilinear Regression\n",
        "\n",
        "Last time we looked at a simple linear regression model $sales = \\beta_0 + \\beta_1\\cdot\\textit{TV advertising budget}$. More generally, a linear model makes a prediction by computing a weighted sum of their input features (plus a constant).\n",
        "\n",
        "**Reading: Chapter 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81lHyTpwx0zu"
      },
      "source": [
        "## Multilinear Regression: Model Assumptions\n",
        "**Model**:\n",
        "\n",
        "$\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 +\\cdots + \\theta_nx_n$\n",
        "1. $\\hat{y}$ is the predicted value.\n",
        "2. $n$ is the number of features.\n",
        "3. $x_i$ is the i-th feature value.\n",
        "4. $\\theta_j$ is the j-th model parameter (associated with $x_j$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azLxHqqUx02U"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb5RGmIdx04v"
      },
      "source": [
        "# Toy example\n",
        "columns = ['Homework', 'Midterm', 'Final']\n",
        "data = pd.DataFrame({\n",
        "    \"Homework\": [95, 70, 80, 100, 70],\n",
        "    \"Midterm\": [90, 60, 80, 80, 85],\n",
        "    \"Final\": [93, 66, 85, 60, 90]\n",
        "}, index=[\"Alice\", \"Bob\", \"Clare\", \"David\", \"Eve\"])\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKguV_HWx09V"
      },
      "source": [
        "In this case:\n",
        "- $x_1$ is the homework feature\n",
        "- $x_2$ is the midterm feature\n",
        "- $y$ is the final feature\n",
        "- model is: $final = \\theta_0 + \\theta_1 * homework + \\theta_2 * midterm$\n",
        "- We need to come up with values for $\\theta_0, \\theta_1, \\theta_2$ to complete the model.\n",
        "\n",
        "**Objective**: Suppose that another student Fred has Homework score 85 and Midterm score 80. What is prediction of his final exam score?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txFgiEHAx0_q"
      },
      "source": [
        "## Multilinear Regression: Vectorized form\n",
        "\n",
        "The multilinear model can also be written as:\n",
        "\n",
        "**$\\hat{y} = \\theta\\cdot\\textbf{x}$**.\n",
        "1. $\\theta = (\\theta_0, \\theta_1, ..., \\theta_n)$ is the paramter vector.\n",
        "2. $\\textbf{x} = (1, x_1, ..., x_n)$ is the feature vector.\n",
        "3. The symbol $\\cdot$ represents the inner-product of two vectors. For example, $(1, 2, 3)\\cdot (4, 5, 6) = 1\\times 4 + 2\\times 5 + 3\\times 6 = 32$.\n",
        "\n",
        "**Why is the expression $\\theta\\cdot\\textbf{x}$ equivalent to $\\theta_0 + \\theta_1x_1 + \\theta_2x_2 +\\cdots + \\theta_nx_n$?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8Acxyvfx1CF"
      },
      "source": [
        "# Let's apply the linear regression tool in sci-kit learn on the toy example\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoD9GMG1x1Em"
      },
      "source": [
        "# Retrieve the estimated parameter values.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwJG44-7x1HE"
      },
      "source": [
        "## Multilinear Regression: Cost Function\n",
        "In order to calculate the best value for each parameter, we need a **cost function** that evaluates the errors made by a give set of parameter values. Here we use the **mean squared error (MSE)** function as the cost function:\n",
        "\n",
        "$J(\\textbf{X}, \\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\big(\\theta\\cdot\\textbf{x}^{(i)} - y^{(i)}\\big)^2$\n",
        "\n",
        "Here $(\\textbf{x}^{(i)}, y^{(i)})$ represents the i-th training example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh-Zy_vEx1JZ"
      },
      "source": [
        "# Calculate the MSE cost of the toy example for the parameter values given by sci-kit learn.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG2rW2SNx1Lw"
      },
      "source": [
        "## Multilinear Regression: Training Algorithm 1\n",
        "The value of $\\theta$ that minimizes the cost function is given by the following **normal equation**:\n",
        "\n",
        "$\\hat{\\theta} = \\big(\\textbf{X}^T\\cdot\\textbf{X}\\big)^{-1}\\cdot\\textbf{X}^T\\cdot\\textbf{y}$.\n",
        "\n",
        "1. $\\textbf{X}$ is an $m\\times (n+1)$ matrix whose i-th row is $\\textbf{x}^{(i)}$.\n",
        "$$\\textbf{X} = \\begin{pmatrix}\n",
        "1 & x^{(1)}_1 & x^{(1)}_2 & \\cdots & x^{(1)}_n \\\\\n",
        "1 & x^{(2)}_1 & x^{(2)}_2 & \\cdots & x^{(2)}_n \\\\\n",
        "\\vdots & \\vdots &\\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x^{(m)}_1 & x^{(m)}_2 & \\cdots & x^{(m)}_n \\\\\n",
        "\\end{pmatrix}$$\n",
        "2. $$\\textbf{y} = \\begin{pmatrix}y^{(1)} \\\\ \\vdots \\\\ y^{(m)}\\end{pmatrix}$$.\n",
        "3. The cost function $J(\\theta)$ also has a matrix expression\n",
        "$$J(\\theta) = \\frac{1}{m}(\\textbf{X}^T\\cdot\\theta - \\textbf{y})^T\\cdot (\\textbf{X}^T\\cdot\\theta - \\textbf{y})$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0r86Sigx1OO"
      },
      "source": [
        "# Construct matrix X using np.hstack(), np.ones()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIZZCIicx1Q7"
      },
      "source": [
        "# Construct vector y\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "197CDh19x1Th"
      },
      "source": [
        "# Apply the normal equation to find theta\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ezXjZlvx1WG"
      },
      "source": [
        "## Multilinear Regression: Training Algorithm 2\n",
        "The normal equation is not applicable when $\\textbf{X}^T\\cdot\\textbf{X}$ is not invertible. It happens if:\n",
        "- Several features are linearly dependent (for example, feature3 = feature1 + feature2)\n",
        "- The number of features is greater than the number of training data (for example, DNA data)\n",
        "\n",
        "When the matrix $\\textbf{X}$ is too large, the normal equation may take too long to finish since it requires a matrix multiplication.\n",
        "\n",
        "In these cases, we can use the **gradient descent** method to minimize the cost function instead.\n",
        "\n",
        "Gradient descent with one variable ideally looks like this:\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/0*fU8XFt-NCMZGAWND.\" width=\"600\">\n",
        "\n",
        "Gradient descent with two variables ideally looks like this:\n",
        "<img src=\"https://blog.paperspace.com/content/images/2019/09/F1-02.large.jpg\" width=\"600\">\n",
        "\n",
        "Gradient descent is an iterative algorithm for finding the **local minimum** of a differentiable function.\n",
        "- Choose an initial value of $\\hat{\\theta}$ and a **learning rate** $r$.\n",
        "- For each iteration $k$, do:\n",
        "$$\\hat{\\theta} \\leftarrow \\hat{\\theta} - r\\cdot\\frac{\\partial J(\\hat{\\theta})}{\\partial \\theta}.$$\n",
        "- The partial derivative of the cost function is given by\n",
        "$$\n",
        "\\frac{\\partial J(\\hat{\\theta})}{\\partial \\theta} = \\frac{2}{m}\\cdot\\textbf{X}^T\\cdot(\\textbf{X}\\cdot\\theta - \\textbf{y}).\n",
        "$$\n",
        "- **Verify the formula of partial derivative asuuming there is one input feature.**\n",
        "\n",
        "- End iteration if certain stop criteria is reached, such as:\n",
        "    - Value of $\\hat{\\theta}$ becomes stable.\n",
        "    - Certain iteration amount is reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clRpPwu-x1Yp"
      },
      "source": [
        "# Choose a random initial value for each parameter.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yP5gFqHx1bM"
      },
      "source": [
        "# Perform gradient descent once.\n",
        "# Choose a learning rate r\n",
        "\n",
        "\n",
        "# 1. Calculate the gradient\n",
        "\n",
        "\n",
        "# 2. Update the parameters\n",
        "\n",
        "\n",
        "# 3. (optional) Show the MSE cost with new parameter values\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPqDCTznx1dq"
      },
      "source": [
        "# Perform gradient descent multiple times\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sfCU6C33reK"
      },
      "source": [
        "# Plot the learning curve.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXOFqVwlx1gQ"
      },
      "source": [
        "**Discussion**\n",
        "1. Change $r$ to 0.000001 and 1. Observe the MSE curve.\n",
        "2. Do the initial parameter values matter?\n",
        "3. How to determine when to stop the iteration?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEnnJd4Qx1ii"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVWfuoUHx1lB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVfSS1dgx1np"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}